{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piEWXaGbqSCi"
      },
      "source": [
        "# **Lab #6**\n",
        "\n",
        "**Wajd Alrabiah**\n",
        "\n",
        "**443007641**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparation:**"
      ],
      "metadata": {
        "id": "xjxbhX6yL7cg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWjggNPtq_fg"
      },
      "outputs": [],
      "source": [
        "# Installation\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeACwjjPrUMN"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 1:** Use Hugging Face Transformers library to generate text based on the input “Hi, we are”.\n",
        "\n",
        "I did follow the instructions on the lab file \"CAI451-Lab6.pdf\" **Citation and all rights for the *code*  reserved to Professor *Badr Aldaihani***\n"
      ],
      "metadata": {
        "id": "wxLdm9prxtwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the task and model name\n",
        "task = \"text-generation\"\n",
        "model_name = \"gpt2\""
      ],
      "metadata": {
        "id": "hcumps2YzYkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the parameters\n",
        "max_output_length = 30\n",
        "num_of_return_sequences = 2\n",
        "input_text = \"Hi, We are\""
      ],
      "metadata": {
        "id": "E0dFwXaKzrO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform text generation\n",
        "text_generator = pipeline(task, model = model_name)\n",
        "generated_text = text_generator(input_text, max_length=max_output_length,\n",
        "num_return_sequences=num_of_return_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg6DeM5-z2TT",
        "outputId": "1ca4f1a1-dfaa-4a3a-f9c9-092b9c9db538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the results\n",
        "for i, result in enumerate(generated_text):\n",
        "    print(f\"Generated Text {i + 1}: {result['generated_text']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFH5mTNU0MNP",
        "outputId": "b76cc9f4-c8d4-4e61-e9d4-cc0b33c0f739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text 1: Hi, We are still on the verge of getting a better look at the new Apple TV. The first test will involve a little bit of tech behind\n",
            "\n",
            "Generated Text 2: Hi, We are a team of professional astronomers from NASA and our European partners at the National University of Iceland. The main goal is to understand cosmic and\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra: User can choose the input as desired\n",
        "input_text = input(\"Kindly enter the desired input text:\")\n",
        "generated_text = text_generator(input_text, max_length=max_output_length,\n",
        "num_return_sequences=num_of_return_sequences)\n",
        "\n",
        "for i, result in enumerate(generated_text):\n",
        "    print(f\"Generated Text {i + 1}: {result['generated_text']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujZ3bcZR3hF7",
        "outputId": "5fb3cb38-6084-42c1-d22e-0748d7bca271"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kindly enter the desired input text:I am an Artificial intelligence developer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text 1: I am an Artificial intelligence developer. I do all of them for fun. I'm not trying to write more stuff for this series. As a matter\n",
            "\n",
            "Generated Text 2: I am an Artificial intelligence developer who will become a full-time software developer. I spend a lot of time on my phone constantly updating the app (\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 2:** Use Hugging Face Transformers library to Perform Named Entity Recognition (NER) using BERT for classifying entities such as person, location, organization and date.\n",
        "\n",
        "I did follow the instructions on the lab file \"CAI451-Lab6.pdf\" **Citation and all rights for the *code*  reserved to Professor *Badr Aldaihani***\n"
      ],
      "metadata": {
        "id": "Z2HFKRmz9n_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model and tokenizer for NER\n",
        "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForTokenClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJr65Vvn-A83",
        "outputId": "abba19de-2cd0-47ae-98dc-53791ef82c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a NER pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "c3eW17g7-Ovz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input sentence\n",
        "sentence = \"Noura has been studying at princess Noura University in Riyadh, Saudi Arabia since 1/1/2022\"\n",
        "\n",
        "# Extra: User can choose the input as desired\n",
        "user_input = input(\"Kindly enter the desired input text:\")"
      ],
      "metadata": {
        "id": "i4stVYTa-SAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform NER\n",
        "ner_results = ner_pipeline(sentence)"
      ],
      "metadata": {
        "id": "FoElZc7X-bvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter and format results\n",
        "formatted_results = []\n",
        "for entity in ner_results:\n",
        "  if entity['entity'] in ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-DATE', 'I-DATE']:\n",
        "    formatted_results.append({\n",
        "    'word': entity['word'],\n",
        "    'entity': entity['entity'],\n",
        "    'score': entity['score']\n",
        "    })"
      ],
      "metadata": {
        "id": "6eDWHXYBCqYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "if formatted_results:\n",
        "    for result in formatted_results:\n",
        "        print(f\"Entity: {result['word']}, Type: {result['entity']}, Confidence: {result['score']:.2f}\")\n",
        "else:\n",
        "    print(\"No relevant entities found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nfI9eeJ_Dgu",
        "outputId": "b2ca66d4-852c-4ed2-f208-5f12e73f9064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: No, Type: I-PER, Confidence: 1.00\n",
            "Entity: ##ura, Type: I-PER, Confidence: 0.99\n",
            "Entity: No, Type: I-ORG, Confidence: 0.99\n",
            "Entity: ##ura, Type: I-ORG, Confidence: 0.98\n",
            "Entity: University, Type: I-ORG, Confidence: 0.98\n",
            "Entity: R, Type: I-LOC, Confidence: 1.00\n",
            "Entity: ##iya, Type: I-LOC, Confidence: 1.00\n",
            "Entity: ##dh, Type: I-LOC, Confidence: 1.00\n",
            "Entity: Saudi, Type: I-LOC, Confidence: 1.00\n",
            "Entity: Arabia, Type: I-LOC, Confidence: 1.00\n"
          ]
        }
      ]
    }
  ]
}